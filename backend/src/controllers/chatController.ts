import { Request, Response } from 'express';
import OpenAI from 'openai';
import { logInfo, logSuccess, logError } from '../middleware/logger';

const systemPrompt = `
Voc√™ √© o 'Pipoqueiro', um assistente de IA amig√°vel, divertido e 
extremamente especialista em cinema e televis√£o. Sua √∫nica fun√ß√£o
√© conversar com usu√°rios sobre filmes, s√©ries, atores, diretores e 
dar recomenda√ß√µes.

REGRAS:
1. NUNCA responda sobre nenhum outro t√≥pico que n√£o seja cinema ou TV.
2. Se o usu√°rio perguntar sobre qualquer outro t√≥pico que n√£o seja cinema ou TV 
   (como pol√≠tica, esportes, matem√°tica, etc.), voc√™ deve gentilmente recusar. 
   Diga a ele que voc√™ n√£o pode falar sobre isso, pois o CR7 e o Messi n√£o deixam.
3. Seja sempre prestativo e use uma linguagem casual, como um amigo
   apaixonado por filmes.
4. Mantenha as respostas relativamente curtas e focadas.
`;

export class ChatController {
  private openai: OpenAI;

  constructor() {
    // Puxa a chave do .env
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  // POST /api/chat
  async handleChatMessage(req: Request, res: Response) {
    try {
      logInfo('üí¨ NOVA MENSAGEM PARA O CHAT');
      const { prompt, historico } = req.body; // Pega a mensagem do usu√°rio
      const userId = (req as any).user.userId; // ID do usu√°rio logado

      if (!process.env.OPENAI_API_KEY) {
        logError('OPENAI_API_KEY n√£o definida no .env');
        return res.status(500).json({
          success: false,
          message: 'Configura√ß√£o do servidor incompleta.'
        });
      }

      if (!prompt) {
        logError('Prompt (mensagem) n√£o fornecido');
        return res.status(400).json({
          success: false,
          message: 'Nenhuma mensagem fornecida.'
        });
      }

      logInfo('Processando mensagem com OpenAI...', [userId, prompt]);

      // Prepara o array de mensagens (incluindo o hist√≥rico, se houver)
      const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
        {
          role: 'system',
          content: systemPrompt, // A regra-mestra!
        },
        // TODO: Voc√™ pode adicionar o hist√≥rico da conversa aqui
        // ...(historico || []), 
        {
          role: 'user',
          content: prompt, // A pergunta atual do usu√°rio
        },
      ];

      // Chama a API da OpenAI
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo', // Modelo r√°pido e eficiente
        messages: messages,
        max_tokens: 200, // Limita o tamanho da resposta
        temperature: 0.7, // Um bom balan√ßo de criatividade
      });

      const aiResponse = completion.choices[0].message?.content;
      logSuccess('Resposta da IA gerada com sucesso');

      res.json({
        success: true,
        message: 'Resposta gerada',
        data: {
          response: aiResponse || 'N√£o consegui pensar em nada... üòÖ'
        }
      });

    } catch (error) {
      logError('‚ùå ERRO AO PROCESSAR CHAT:', error);
      res.status(500).json({
        success: false,
        message: 'Erro interno do servidor ao processar chat'
      });
    }
  }
}